{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21ac9d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names: Caylum Hurley (22356363), Joe Considine (22344977), Darragh Quinn (22359621)\n",
    "# Code executes to the end without errors\n",
    "# Links to third-party implementations used for the submission: https://ale.farama.org/environments/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdbd1ae",
   "metadata": {},
   "source": [
    "Names: Caylum Hurley (22356363), Joe Considine (22344977), Darragh Quinn (22359621)\n",
    "Code executes to the end without errors\n",
    "Links to third-party implementations used for the submission: https://ale.farama.org/environments/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "174b4537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ale-py in c:\\users\\joeco\\appdata\\roaming\\python\\python312\\site-packages (0.11.2)\n",
      "Requirement already satisfied: gymnasium[accept-rom-license,atari] in c:\\users\\joeco\\appdata\\roaming\\python\\python312\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\joeco\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium[accept-rom-license,atari]) (2.0.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\joeco\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium[accept-rom-license,atari]) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\joeco\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\joeco\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "#Install gynasium with Atari support\n",
    "\n",
    "%pip install gymnasium[atari,accept-rom-license] ale-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bced9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496761d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Registering the Atari environments from the ALE\n",
    "\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c49f31",
   "metadata": {},
   "source": [
    "Creating the Pong environment. The rgb_array captures frames for preprocessing and video generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6af46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape:  (210, 160, 3)\n",
      "Action shape:  Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "#Creates Pong environment\n",
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "#Reset the enviornment for initial observation\n",
    "observation, info = env.reset()\n",
    "\n",
    "#Verifying setup\n",
    "print(\"Observation shape: \", observation.shape)\n",
    "print(\"Action shape: \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf350f",
   "metadata": {},
   "source": [
    "Preprocessing Observation. Since the raw images given are RGB images, we need to convert each frame to grayscale, resize to 84*84 pixels, and normalise pixel values to a range [0, 1]. Doing this makes learning efficiency much better as opposed to processing the images directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4297e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(observation):\n",
    "    gray = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY) #RGB to grayscale\n",
    "    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA) #Resize to DQN input size\n",
    "    normalised = resized / 255.0 #Normalise pixel values\n",
    "    \n",
    "    return normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c0542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed shape:  (84, 84)\n",
      "Minimum pixel value:  0.25098039215686274\n",
      "Maximum pixel value:  0.7019607843137254\n"
     ]
    }
   ],
   "source": [
    "#TODO should min and max be 0 and 1?\n",
    "#Test preprocessing\n",
    "\n",
    "processed_observation = preprocess_observation(observation)\n",
    "\n",
    "print(\"Processed shape: \", processed_observation.shape)\n",
    "print(\"Minimum pixel value: \", processed_observation.min())\n",
    "print(\"Maximum pixel value: \", processed_observation.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d374775",
   "metadata": {},
   "source": [
    "Our DQN takes the preprocessed game frame as the input and outputs a Q-value for each possible action. The action with the highest Q-value is chosen as the agent's policy action during exploitation. Our architecture uses three convolutional layers for spatial feature extraction, two fully connected layers for decision making, and a final linear output that produces our Q-value.\n",
    "\n",
    "Convolutional layers detect the ball and the paddles in their relative positions.\n",
    "\n",
    "ReLU used to introduce non-linearity so that the model can learn more complex scenarios.\n",
    "\n",
    "Fully connected layers combine the extracted features for a Q-value estimate.\n",
    "\n",
    "Output layer has one neuron per action, producing the award for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edaa811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        #Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, # Input is a single, preprocessed grayscale image of shape (1, 84, 84)\n",
    "            out_channels=32,\n",
    "            kernel_size=8,\n",
    "            stride=4\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=4,\n",
    "            stride=2\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            stride=1\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(7*7*64, 512) #Feature map size (7*7*64)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, n_actions) #One Q-value per action\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x = x.view(x.size(0), -1) #Flatten tensor for fully connected layers\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        q_values = self.fc2(x)\n",
    "        \n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d939c2a",
   "metadata": {},
   "source": [
    "Creating Target and Online Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1d565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "#Number of actions\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "#Creates Q-network\n",
    "policy_net = DQN(n_actions).to(device)\n",
    "\n",
    "#Creates target Q-network\n",
    "target_net = DQN(n_actions).to(device)\n",
    "\n",
    "#Copy weights from policy to target network\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "#Setting target mode to evaluation mode\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394edd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values:  tensor([[ 0.0205, -0.0183,  0.0101, -0.0199,  0.0369, -0.0372]])\n",
      "Q-values shape:  torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "#Converting a preprocessed frame into a tensor\n",
    "state_tensor = torch.tensor(processed_observation, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "#Pass state through online network\n",
    "with torch.no_grad():\n",
    "    q_values = policy_net(state_tensor)\n",
    "    \n",
    "#Print Q-Values\n",
    "print(\"Q-values: \", q_values)\n",
    "print(\"Q-values shape: \", q_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11bc0bb",
   "metadata": {},
   "source": [
    "We are using a Replay Buffer to store transitions explored by the agent in the environment. Replay is very important for stabilising DQN training. The agent samples random mini-batches from the replay buffer. Doing it this way improves data efficiency by reusing experiences and it breaks temporal correlations between consecutive frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done)) #Appends a tuple\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef76ebb",
   "metadata": {},
   "source": [
    "We are giving the replay buffer a large capacity to ensure a diverse set of experiences are available for training. When buffer is filled, older experiences are discarded automatically. This reduces overfitting and makes training more stable.\n",
    "\n",
    "At the start of training, the replay buffer does not contain enough transitions to form a mini-batch. Training updates are therefore skipped until the buffer has a sufficient number of experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958469f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer size:  1\n",
      "Not enough samples in replay buffer to sample a batch yet\n",
      "Replay buffer size:  2\n",
      "Sampled batch size:  2\n",
      "Sampled actions:  (np.int64(1), np.int64(5))\n",
      "Replay buffer size:  3\n",
      "Sampled batch size:  2\n",
      "Sampled actions:  (np.int64(1), np.int64(5))\n",
      "Replay buffer size:  4\n",
      "Sampled batch size:  2\n",
      "Sampled actions:  (np.int64(5), np.int64(4))\n",
      "Replay buffer size:  5\n",
      "Sampled batch size:  2\n",
      "Sampled actions:  (np.int64(4), np.int64(5))\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(capacity=1000)\n",
    "\n",
    "for i in range(5):\n",
    "    replay_buffer.push(\n",
    "        processed_observation,\n",
    "        env.action_space.sample(),\n",
    "        1.0,\n",
    "        processed_observation,\n",
    "        False\n",
    "    )\n",
    "    \n",
    "    print(\"Replay buffer size: \", len(replay_buffer))\n",
    "    \n",
    "    batch_size = 2 # must be <= buffer size\n",
    "    \n",
    "    if len(replay_buffer) >= batch_size:\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "        print(\"Sampled batch size: \", len(states))\n",
    "        print(\"Sampled actions: \", actions)\n",
    "    else: \n",
    "        print(\"Not enough samples in replay buffer to sample a batch yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10b35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "learning_rate = 1e-4\n",
    "target_update = 1000\n",
    "epsilon = 1.0",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "steps_completed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57572494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e14255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step():\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    states = torch.cat(states).to(device)\n",
    "    next_states = torch.cat(next_states).to(device)\n",
    "\n",
    "    actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    q_vals = policy_net(states).gather(1, actions)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        max_next_q_values = target_net(next_states).max(1)[0].unsqueeze(1)\n",
    "        target_q_vals = rewards + (gamma * max_next_q_values * (1 - dones))\n",
    "\n",
    "    loss = F.mse_loss(q_vals, target_q_vals)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58686887",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    observation, info = env.reset()\n",
    "    state = preprocess_observation(observation)\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        steps_completed += 1\n",
    "\n",
    "        action = choose_action(state, epsilon)\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        next_state = preprocess_observation(next_observation)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            training_step()\n",
    "\n",
    "        if steps_completed % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf1654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Markdown cell for Learning update (Will update soon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
