{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebdbd1ae",
   "metadata": {},
   "source": [
    "Names: Caylum Hurley (22356363), Joe Considine (22344977), Darragh Quinn (22359621)\n",
    "\n",
    "Code executes to the end without errors\n",
    "\n",
    "Links to third-party implementations used for the submission: https://ale.farama.org/environments/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "174b4537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ale-py in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: gymnasium[accept-rom-license,atari] in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (2.2.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opencv-python) (2.2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (12.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Install gynasium with Atari support\n",
    "\n",
    "%pip install gymnasium[atari,accept-rom-license] ale-py\n",
    "%pip install --upgrade pip\n",
    "%pip install opencv-python\n",
    "%pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2bced9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n",
      "3.12.9 (tags/v3.12.9:fdb8142, Feb  4 2025, 15:27:58) [MSC v.1942 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "stack_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "496761d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Registering the Atari environments from the ALE\n",
    "\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c49f31",
   "metadata": {},
   "source": [
    "Creating the Pong environment. The rgb_array captures frames for preprocessing and video generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6f6af46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape:  (210, 160, 3)\n",
      "Action shape:  Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "#Creates Pong environment\n",
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"rgb_array\")\n",
    "\n",
    "#Reset the enviornment for initial observation\n",
    "observation, info = env.reset()\n",
    "\n",
    "#Verifying setup\n",
    "print(\"Observation shape: \", observation.shape)\n",
    "print(\"Action shape: \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf350f",
   "metadata": {},
   "source": [
    "Preprocessing Observation. Since the raw images given are RGB images, we need to convert each frame to grayscale, resize to 84*84 pixels, and normalise pixel values to a range [0, 1]. Doing this makes learning efficiency much better as opposed to processing the images directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5b4297e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(observation):\n",
    "    gray = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY) #RGB to grayscale\n",
    "    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA) #Resize to DQN input size\n",
    "    normalised = resized / 255.0 #Normalise pixel values\n",
    "    \n",
    "    return normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c0542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed shape:  (84, 84)\n",
      "Minimum pixel value:  0.25098039215686274\n",
      "Maximum pixel value:  0.7019607843137254\n"
     ]
    }
   ],
   "source": [
    "#Test preprocessing\n",
    "\n",
    "processed_observation = preprocess_observation(observation)\n",
    "\n",
    "print(\"Processed shape: \", processed_observation.shape)\n",
    "print(\"Minimum pixel value: \", processed_observation.min())\n",
    "print(\"Maximum pixel value: \", processed_observation.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d374775",
   "metadata": {},
   "source": [
    "Our DQN takes the preprocessed game frame as the input and outputs a Q-value for each possible action. The action with the highest Q-value is chosen as the agent's policy action during exploitation. Our architecture uses three convolutional layers for spatial feature extraction, two fully connected layers for decision making, and a final linear output that produces our Q-value.\n",
    "\n",
    "Convolutional layers detect the ball and the paddles in their relative positions.\n",
    "\n",
    "ReLU used to introduce non-linearity so that the model can learn more complex scenarios.\n",
    "\n",
    "Fully connected layers combine the extracted features for a Q-value estimate.\n",
    "\n",
    "Output layer has one neuron per action, producing the award for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0edaa811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        #Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=stack_size, # Input is a single, preprocessed grayscale image of shape (1, 84, 84)\n",
    "            out_channels=32,\n",
    "            kernel_size=8,\n",
    "            stride=4\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=4,\n",
    "            stride=2\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=64,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "            stride=1\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(7*7*64, 512) #Feature map size (7*7*64)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, n_actions) #One Q-value per action\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        x = x.view(x.size(0), -1) #Flatten tensor for fully connected layers\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        q_values = self.fc2(x)\n",
    "        \n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d939c2a",
   "metadata": {},
   "source": [
    "Creating Target and Online Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "24a1d565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "#Number of actions\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "#Creates Q-network\n",
    "policy_net = DQN(n_actions).to(device)\n",
    "\n",
    "#Creates target Q-network\n",
    "target_net = DQN(n_actions).to(device)\n",
    "\n",
    "#Copy weights from policy to target network\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "#Setting target mode to evaluation mode\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "394edd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 84, 84])\n",
      "Q-values:  tensor([[ 0.0061, -0.0300,  0.0405, -0.0267, -0.0144, -0.0046]])\n",
      "Q-values shape:  torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "#Converting a preprocessed frame into a tensor\n",
    "state_tensor = torch.tensor(np.stack([processed_observation]*stack_size, axis=0), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "print(state_tensor.shape)  # should be (1, 4, 84, 84)\n",
    "\n",
    "#Pass state through online network\n",
    "with torch.no_grad():\n",
    "    q_values = policy_net(state_tensor)\n",
    "    \n",
    "#Print Q-Values\n",
    "print(\"Q-values: \", q_values)\n",
    "print(\"Q-values shape: \", q_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11bc0bb",
   "metadata": {},
   "source": [
    "We are using a Replay Buffer to store transitions explored by the agent in the environment. Replay is very important for stabilising DQN training. The agent samples random mini-batches from the replay buffer. Doing it this way improves data efficiency by reusing experiences and it breaks temporal correlations between consecutive frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "13c6ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done)) #Appends a tuple\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef76ebb",
   "metadata": {},
   "source": [
    "We are giving the replay buffer a large capacity to ensure a diverse set of experiences are available for training. When buffer is filled, older experiences are discarded automatically. This reduces overfitting and makes training more stable.\n",
    "\n",
    "At the start of training, the replay buffer does not contain enough transitions to form a mini-batch. Training updates are therefore skipped until the buffer has a sufficient number of experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "958469f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer size:  1\n",
      "Not enough samples in replay buffer to sample a batch yet\n",
      "Replay buffer size:  2\n",
      "Sampled batch size:  2\n",
      "Sampled actions:  (np.int64(0), np.int64(2))\n",
      "Replay buffer size:  3\n",
      "Sampled batch size:  2\n",
      "Sampled actions:  (np.int64(0), np.int64(2))\n",
      "Replay buffer size:  4\n",
      "Sampled batch size:  2\n",
      "Sampled actions:  (np.int64(0), np.int64(5))\n",
      "Replay buffer size:  5\n",
      "Sampled batch size:  2\n",
      "Sampled actions:  (np.int64(2), np.int64(5))\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(capacity=1000)\n",
    "\n",
    "for i in range(5):\n",
    "    stacked_test_frame = np.stack([processed_observation]*stack_size, axis=0)\n",
    "    replay_buffer.push(\n",
    "        stacked_test_frame,\n",
    "        env.action_space.sample(),\n",
    "        1.0,\n",
    "        stacked_test_frame,\n",
    "        False\n",
    "    )\n",
    "    \n",
    "    print(\"Replay buffer size: \", len(replay_buffer))\n",
    "    \n",
    "    batch_size = 2 # must be <= buffer size\n",
    "    \n",
    "    if len(replay_buffer) >= batch_size:\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "        print(\"Sampled batch size: \", len(states))\n",
    "        print(\"Sampled actions: \", actions)\n",
    "    else: \n",
    "        print(\"Not enough samples in replay buffer to sample a batch yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b10b35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "learning_rate = 1e-4\n",
    "target_update = 1000\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "steps_completed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8e14255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            return policy_net(state_tensor).argmax(dim=1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c480215",
   "metadata": {},
   "source": [
    "We reduced maximisation bias with Double DQN in the below cell. The policy network picks the next best action while the target network evaluates the value of the chosen action. The result is a more accurate target estimate and greater training stability. This leads to much more consitent performance in our Pong environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be68dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def training_step():\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    states = [torch.tensor(state, dtype=torch.float32).unsqueeze(0) if state.ndim == 2 else torch.tensor(state, dtype=torch.float32) for state in states]\n",
    "    next_states = [torch.tensor(state, dtype=torch.float32).unsqueeze(0) if state.ndim == 2 else torch.tensor(state, dtype=torch.float32) for state in next_states]\n",
    "\n",
    "    \n",
    "    # Concatenate tensors\n",
    "    states = torch.stack(states).to(device)\n",
    "    next_states = torch.stack(next_states).to(device)\n",
    "    \n",
    "    actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.bool).unsqueeze(1).to(device)\n",
    "    \n",
    "    dones = dones.float()\n",
    "    \n",
    "    # Continue with the rest of the training logic...\n",
    "\n",
    "    q_vals = policy_net(states).gather(1, actions)\n",
    "    \n",
    "    #Double DQN to fix maximization bias\n",
    "    with torch.no_grad():\n",
    "        next_actions = policy_net(next_states).argmax(dim=1, keepdim=True) # Get actions from policy network\n",
    "        next_q_vals = target_net(next_states).gather(1, next_actions) # Action evaluation from target network\n",
    "        target_q_vals = rewards + (gamma * next_q_vals * (1 - dones))\n",
    "\n",
    "    loss = F.mse_loss(q_vals, target_q_vals)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "58686887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -21.0, Epsilon: 0.995\n",
      "Episode 10, Total Reward: -20.0, Epsilon: 0.946\n",
      "Episode 20, Total Reward: -21.0, Epsilon: 0.900\n",
      "Episode 30, Total Reward: -21.0, Epsilon: 0.856\n",
      "Episode 40, Total Reward: -18.0, Epsilon: 0.814\n",
      "Episode 50, Total Reward: -21.0, Epsilon: 0.774\n",
      "Episode 60, Total Reward: -19.0, Epsilon: 0.737\n",
      "Episode 70, Total Reward: -21.0, Epsilon: 0.701\n",
      "Episode 80, Total Reward: -20.0, Epsilon: 0.666\n",
      "Episode 90, Total Reward: -20.0, Epsilon: 0.634\n"
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    observation, info = env.reset()\n",
    "    \n",
    "    stacked_frames = deque(maxlen=stack_size)\n",
    "\n",
    "    frame = preprocess_observation(observation)\n",
    "\n",
    "    for _ in range(stack_size):\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "    state = np.stack(stacked_frames, axis=0)  # Shape: (stack_size, 84, 84)\n",
    "\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        steps_completed += 1\n",
    "\n",
    "        action = choose_action(state, epsilon)\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        next_frame = preprocess_observation(next_observation)\n",
    "        stacked_frames.append(next_frame)\n",
    "        next_state = np.stack(stacked_frames, axis=0)\n",
    "\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            training_step()\n",
    "\n",
    "        if steps_completed % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    episode_rewards.append(total_reward)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8aeaa9",
   "metadata": {},
   "source": [
    "Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episode Reward Plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"DQN Training Performance on Pong\")\n",
    "plt.show()\n",
    "\n",
    "#Flat early region indicates random behaviour.\n",
    "#Eventually the model begins to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d052da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothed Rewards of Results\n",
    "\n",
    "def smooth_rewards(data, window_size=50): #Moving average smoothing function for better visualisation\n",
    "    smoothed = np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "    return smoothed\n",
    "\n",
    "smoothed_rewards = smooth_rewards(episode_rewards, window_size=50)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Average Reward of Last 50 Episodes\")\n",
    "plt.title(\"Smoothed DQN Performance on Pong (Moving Average)\")\n",
    "\n",
    "#Necessary Plot to remove randomness\n",
    "#We can see the learning trend and the agents improvement from this plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3d0225",
   "metadata": {},
   "source": [
    "Evaluation of the plots\n",
    "\n",
    "At the beginning of the of the graph, the learning curve shows rewards close to or equal to -21, which is expected in early DQN stages because of the high exploration, low exploitation, and sparse delayed rewards in our Pong environment. As training progresses and as the epsilon value decays, the agent starts exploiting learned Q-values more often, leading to slightly better results and improvements.\n",
    "\n",
    "We made a moving average plot so that we can focus on performance trends. The purpose of a moving average is to smooth short term variance caused by exploration. The gradual upward trend indicates that it is learning a more effective policy over time as it improves at the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6bca8",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "Double DQN - https://www.emergentmind.com/topics/double-q-learning-algorithm\n",
    "\n",
    "Plot logic - https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
